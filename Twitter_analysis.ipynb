{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPy3HN0Uutb36o5+FidUI4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelshah2409/Twitter-Sentiment-Analysis/blob/main/Twitter_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR0qqndwA4G_"
      },
      "source": [
        "# Twitter Sentiment Analysis \n",
        "#### Analyzing sentiments is a task of natural language processing. All the social media platforms need to keep a check on the sentiments of people engaged in a discussion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orsf9_iVwZjG",
        "outputId": "6d9cab6d-80b8-4e6a-85fc-313c287e323e"
      },
      "source": [
        "\n",
        "#importing the libraries required\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import re\n",
        "import nltk\n",
        "import nltk\n",
        "#reading the dataset\n",
        "data = pd.read_csv('Twitter_Data.csv')\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          clean_text  category\n",
            "0  when modi promised â€œminimum government maximum...      -1.0\n",
            "1  talk all the nonsense and continue all the dra...       0.0\n",
            "2  what did just say vote for modi  welcome bjp t...       1.0\n",
            "3  asking his supporters prefix chowkidar their n...       1.0\n",
            "4  answer who among these the most powerful world...       1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg0IYdVKO5UU",
        "outputId": "d7e9ecab-7f43-4aee-9d7c-bd690520e8db"
      },
      "source": [
        "#analysing the dataset\n",
        "data.info()\n",
        "\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 162980 entries, 0 to 162979\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype  \n",
            "---  ------      --------------   -----  \n",
            " 0   clean_text  162980 non-null  object \n",
            " 1   Positive    162980 non-null  float64\n",
            " 2   Negative    162980 non-null  float64\n",
            " 3   Neutral     162980 non-null  float64\n",
            "dtypes: float64(3), object(1)\n",
            "memory usage: 5.0+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(162980, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYm5tZpkwrhL",
        "outputId": "5fd591c8-3821-4481-f1aa-6e0d6980ea03"
      },
      "source": [
        "#here the dataset contains lot of grammatical/language errors and special characters so we need to clean up them\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stopword=set(stopwords.words('english'))\n",
        "\n",
        "def clean(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text=\" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text=\" \".join(text)\n",
        "    return text\n",
        "\n",
        "data[\"clean_text\"] = data[\"clean_text\"].apply(clean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DeVh3E6JPD-",
        "outputId": "dbff7a10-b29b-4ff0-947f-eee9a47c7ad7"
      },
      "source": [
        "#Calculating the sentiment scores of these tweets\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sentiments = SentimentIntensityAnalyzer()\n",
        "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"clean_text\"]]\n",
        "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"clean_text\"]]\n",
        "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"clean_text\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENuZv25ANpih",
        "outputId": "847dd91e-cd5d-4746-9c3b-358a23c1effc"
      },
      "source": [
        "\n",
        "data = data[[\"clean_text\", \"Positive\", \n",
        "             \"Negative\", \"Neutral\"]]\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          clean_text  ...  Neutral\n",
            "0  modi promis â€œminimum govern maximum governance...  ...    0.889\n",
            "1              talk nonsens continu drama vote modi   ...    1.000\n",
            "2  say vote modi  welcom bjp told rahul main camp...  ...    0.805\n",
            "3  ask support prefix chowkidar name modi great s...  ...    0.630\n",
            "4  answer among power world leader today trump pu...  ...    1.000\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVSprWZLN_eI",
        "outputId": "16850c28-fb94-49d5-8a6b-f3350267eb7b"
      },
      "source": [
        "#looking at the most frequent scores \n",
        "x = sum(data[\"Positive\"])\n",
        "y = sum(data[\"Negative\"])\n",
        "z = sum(data[\"Neutral\"])\n",
        "\n",
        "def sentiment_score(a, b, c):\n",
        "    if (a>b) and (a>c):\n",
        "        print(\"Positive ðŸ˜Š \")\n",
        "    elif (b>a) and (b>c):\n",
        "        print(\"Negative ðŸ˜  \")\n",
        "    else:\n",
        "        print(\"Neutral ðŸ™‚ \")\n",
        "sentiment_score(x, y, z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutral ðŸ™‚ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4GV7lRIOg0_",
        "outputId": "9396aa1f-55a9-45a2-c1cf-808c9b0991e2"
      },
      "source": [
        "#total number of sentiment scores\n",
        "print(\"Positive: \", x)\n",
        "print(\"Negative: \", y)\n",
        "print(\"Neutral: \", z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive:  22450.175999999785\n",
            "Negative:  15376.716999999946\n",
            "Neutral:  125074.11200000446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U-aFnd1OkaV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}